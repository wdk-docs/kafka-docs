# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2019, BandCap
# This file is distributed under the same license as the kafka docs package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: kafka docs \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-07-23 14:46+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../source/kafka/documentation.rst:1
msgid "Configuring Connectors"
msgstr "Configuring Connectors"

#: ../../source/kafka/documentation.rst:3
msgid ""
"Connector configurations are simple key-value mappings. For standalone "
"mode these are defined in a properties file and passed to the Connect "
"process on the command line. In distributed mode, they will be included "
"in the JSON payload for the request that creates (or modifies) the "
"connector."
msgstr "Connector configurations are simple key-value mappings. For standalone mode these are defined in a properties file and passed to the Connect process on the command line. In distributed mode, they will be included in the JSON payload for the request that creates (or modifies) the connector."

#: ../../source/kafka/documentation.rst:9
msgid ""
"Most configurations are connector dependent, so they can’t be outlined "
"here. However, there are a few common options:"
msgstr "Most configurations are connector dependent, so they can’t be outlined here. However, there are a few common options:"

#: ../../source/kafka/documentation.rst:20
msgid ""
"The connector.class config supports several formats: the full name or "
"alias of the class for this connector. If the connector is "
"org.apache.kafka.connect.file.FileStreamSinkConnector, you can either "
"specify this full name or use FileStreamSink or FileStreamSinkConnector "
"to make the configuration a bit shorter."
msgstr "The connector.class config supports several formats: the full name or alias of the class for this connector. If the connector is org.apache.kafka.connect.file.FileStreamSinkConnector, you can either specify this full name or use FileStreamSink or FileStreamSinkConnector to make the configuration a bit shorter."

#: ../../source/kafka/documentation.rst:26
msgid ""
"Sink connectors also have a few additional options to control their "
"input. Each sink connector must set one of the following:"
msgstr "Sink connectors also have a few additional options to control their input. Each sink connector must set one of the following:"

#: ../../source/kafka/documentation.rst:34
msgid ""
"For any other options, you should consult the documentation for the "
"connector. Transformations"
msgstr "For any other options, you should consult the documentation for the connector. Transformations"

#: ../../source/kafka/documentation.rst:37
msgid ""
"Connectors can be configured with transformations to make lightweight "
"message-at-a-time modifications. They can be convenient for data "
"massaging and event routing."
msgstr "Connectors can be configured with transformations to make lightweight message-at-a-time modifications. They can be convenient for data massaging and event routing."

#: ../../source/kafka/documentation.rst:41
msgid "A transformation chain can be specified in the connector configuration."
msgstr "A transformation chain can be specified in the connector configuration."

#: ../../source/kafka/documentation.rst:49
msgid ""
"For example, lets take the built-in file source connector and use a "
"transformation to add a static field."
msgstr "For example, lets take the built-in file source connector and use a transformation to add a static field."

#: ../../source/kafka/documentation.rst:52
msgid ""
"Throughout the example we’ll use schemaless JSON data format. To use "
"schemaless format, we changed the following two lines in connect-"
"standalone.properties from true to false: 1 2"
msgstr "Throughout the example we’ll use schemaless JSON data format. To use schemaless format, we changed the following two lines in connect-standalone.properties from true to false: 1 2"

#: ../../source/kafka/documentation.rst:56
msgid "key.converter.schemas.enable value.converter.schemas.enable"
msgstr "key.converter.schemas.enable value.converter.schemas.enable"

#: ../../source/kafka/documentation.rst:58
msgid ""
"The file source connector reads each line as a String. We will wrap each "
"line in a Map and then add a second field to identify the origin of the "
"event. To do this, we use two transformations:"
msgstr "The file source connector reads each line as a String. We will wrap each line in a Map and then add a second field to identify the origin of the event. To do this, we use two transformations:"

#: ../../source/kafka/documentation.rst:67
msgid ""
"After adding the transformations, connect-file-source.properties file "
"looks as following: 1 2 3 4 5 6 7 8 9 10 11"
msgstr "After adding the transformations, connect-file-source.properties file looks as following: 1 2 3 4 5 6 7 8 9 10 11"

#: ../../source/kafka/documentation.rst:70
msgid ""
"name=local-file-source connector.class=FileStreamSource tasks.max=1 "
"file=test.txt topic=connect-test transforms=MakeMap, InsertSource "
"transforms.MakeMap.type=org.apache.kafka.connect.transforms.HoistField\\ "
":math:`Value transforms.MakeMap.field=line "
"transforms.InsertSource.type=org.apache.kafka.connect.transforms.InsertField`\\"
" Value transforms.InsertSource.static.field=data_source "
"transforms.InsertSource.static.value=test-file-source"
msgstr ""

#: ../../source/kafka/documentation.rst:76
msgid ""
"All the lines starting with transforms were added for the "
"transformations. You can see the two transformations we created: "
"“InsertSource” and “MakeMap” are aliases that we chose to give the "
"transformations. The transformation types are based on the list of built-"
"in transformations you can see below. Each transformation type has "
"additional configuration: HoistField requires a configuration called "
"“field”, which is the name of the field in the map that will include the "
"original String from the file. InsertField transformation lets us specify"
" the field name and the value that we are adding."
msgstr ""

#: ../../source/kafka/documentation.rst:86
msgid ""
"When we ran the file source connector on my sample file without the "
"transformations, and then read them using kafka-console-consumer.sh, the "
"results were: 1 2 3"
msgstr "When we ran the file source connector on my sample file without the transformations, and then read them using kafka-console-consumer.sh, the results were: 1 2 3"

#: ../../source/kafka/documentation.rst:90
msgid "“foo” “bar” “hello world”"
msgstr "“foo” “bar” “hello world”"

#: ../../source/kafka/documentation.rst:92
msgid ""
"We then create a new file connector, this time after adding the "
"transformations to the configuration file. This time, the results will "
"be: 1 2 3"
msgstr "We then create a new file connector, this time after adding the transformations to the configuration file. This time, the results will be: 1 2 3"

#: ../../source/kafka/documentation.rst:96
msgid ""
"{“line”:“foo”,“data_source”:“test-file-source”} "
"{“line”:“bar”,“data_source”:“test-file-source”} {“line”:“hello "
"world”,“data_source”:“test-file-source”}"
msgstr "{“line”:“foo”,“data_source”:“test-file-source”} {“line”:“bar”,“data_source”:“test-file-source”} {“line”:“hello world”,“data_source”:“test-file-source”}"

#: ../../source/kafka/documentation.rst:100
msgid ""
"You can see that the lines we’ve read are now part of a JSON map, and "
"there is an extra field with the static value we specified. This is just "
"one example of what you can do with transformations."
msgstr "You can see that the lines we’ve read are now part of a JSON map, and there is an extra field with the static value we specified. This is just one example of what you can do with transformations."

#: ../../source/kafka/documentation.rst:104
msgid ""
"Several widely-applicable data and routing transformations are included "
"with Kafka Connect:"
msgstr "Several widely-applicable data and routing transformations are included with Kafka Connect:"

#: ../../source/kafka/documentation.rst:119
msgid ""
"Details on how to configure each transformation are listed below: "
"org.apache.kafka.connect.transforms.InsertField Insert field(s) using "
"attributes from the record metadata or a configured static value."
msgstr "Details on how to configure each transformation are listed below: org.apache.kafka.connect.transforms.InsertField Insert field(s) using attributes from the record metadata or a configured static value."

#: ../../source/kafka/documentation.rst:123
msgid ""
"Use the concrete transformation type designed for the record key "
"(org.apache.kafka.connect.transforms.InsertField:math:`Key) or value "
"(org.apache.kafka.connect.transforms.InsertField`\\ Value)."
msgstr "Use the concrete transformation type designed for the record key (org.apache.kafka.connect.transforms.InsertField:math:`Key) or value (org.apache.kafka.connect.transforms.InsertField`\\ Value)."

#: ../../source/kafka/documentation.rst:126
msgid ""
"Name Description Type Default Valid Values Importance offset.field Field "
"name for Kafka offset - only applicable to sink connectors. Suffix with !"
" to make this a required field, or ? to keep it optional (the default). "
"string null medium partition.field Field name for Kafka partition. Suffix"
" with ! to make this a required field, or ? to keep it optional (the "
"default). string null medium static.field Field name for static data "
"field. Suffix with ! to make this a required field, or ? to keep it "
"optional (the default). string null medium static.value Static field "
"value, if field name configured. string null medium timestamp.field Field"
" name for record timestamp. Suffix with ! to make this a required field, "
"or ? to keep it optional (the default). string null medium topic.field "
"Field name for Kafka topic. Suffix with ! to make this a required field, "
"or ? to keep it optional (the default). string null medium "
"org.apache.kafka.connect.transforms.ReplaceField Filter or rename fields."
msgstr ""

#: ../../source/kafka/documentation.rst:142
msgid ""
"Use the concrete transformation type designed for the record key "
"(org.apache.kafka.connect.transforms.ReplaceField:math:`Key) or value "
"(org.apache.kafka.connect.transforms.ReplaceField`\\ Value)."
msgstr "Use the concrete transformation type designed for the record key (org.apache.kafka.connect.transforms.ReplaceField:math:`Key) or value (org.apache.kafka.connect.transforms.ReplaceField`\\ Value)."

#: ../../source/kafka/documentation.rst:145
msgid ""
"Name Description Type Default Valid Values Importance blacklist Fields to"
" exclude. This takes precedence over the whitelist. list \"\" medium "
"renames Field rename mappings. list \"\" list of colon-delimited pairs, "
"e.g. foo:bar,abc:xyz medium whitelist Fields to include. If specified, "
"only these fields will be used. list \"\" medium "
"org.apache.kafka.connect.transforms.MaskField Mask specified fields with "
"a valid null value for the field type (i.e. 0, false, empty string, and "
"so on)."
msgstr ""

#: ../../source/kafka/documentation.rst:154
msgid ""
"Use the concrete transformation type designed for the record key "
"(org.apache.kafka.connect.transforms.MaskField:math:`Key) or value "
"(org.apache.kafka.connect.transforms.MaskField`\\ Value)."
msgstr "Use the concrete transformation type designed for the record key (org.apache.kafka.connect.transforms.MaskField:math:`Key) or value (org.apache.kafka.connect.transforms.MaskField`\\ Value)."

#: ../../source/kafka/documentation.rst:157
msgid ""
"Name Description Type Default Valid Values Importance fields Names of "
"fields to mask. list non-empty list high "
"org.apache.kafka.connect.transforms.ValueToKey Replace the record key "
"with a new key formed from a subset of fields in the record value."
msgstr "Name Description Type Default Valid Values Importance fields Names of fields to mask. list non-empty list high org.apache.kafka.connect.transforms.ValueToKey Replace the record key with a new key formed from a subset of fields in the record value."

#: ../../source/kafka/documentation.rst:162
msgid ""
"Name Description Type Default Valid Values Importance fields Field names "
"on the record value to extract as the record key. list non-empty list "
"high org.apache.kafka.connect.transforms.HoistField Wrap data using the "
"specified field name in a Struct when schema present, or a Map in the "
"case of schemaless data."
msgstr "Name Description Type Default Valid Values Importance fields Field names on the record value to extract as the record key. list non-empty list high org.apache.kafka.connect.transforms.HoistField Wrap data using the specified field name in a Struct when schema present, or a Map in the case of schemaless data."

#: ../../source/kafka/documentation.rst:168
msgid ""
"Use the concrete transformation type designed for the record key "
"(org.apache.kafka.connect.transforms.HoistField:math:`Key) or value "
"(org.apache.kafka.connect.transforms.HoistField`\\ Value)."
msgstr "Use the concrete transformation type designed for the record key (org.apache.kafka.connect.transforms.HoistField:math:`Key) or value (org.apache.kafka.connect.transforms.HoistField`\\ Value)."

#: ../../source/kafka/documentation.rst:171
msgid ""
"Name Description Type Default Valid Values Importance field Field name "
"for the single field that will be created in the resulting Struct or Map."
" string medium org.apache.kafka.connect.transforms.ExtractField Extract "
"the specified field from a Struct when schema present, or a Map in the "
"case of schemaless data. Any null values are passed through unmodified."
msgstr "Name Description Type Default Valid Values Importance field Field name for the single field that will be created in the resulting Struct or Map. string medium org.apache.kafka.connect.transforms.ExtractField Extract the specified field from a Struct when schema present, or a Map in the case of schemaless data. Any null values are passed through unmodified."

#: ../../source/kafka/documentation.rst:178
msgid ""
"Use the concrete transformation type designed for the record key "
"(org.apache.kafka.connect.transforms.ExtractField:math:`Key) or value "
"(org.apache.kafka.connect.transforms.ExtractField`\\ Value)."
msgstr "Use the concrete transformation type designed for the record key (org.apache.kafka.connect.transforms.ExtractField:math:`Key) or value (org.apache.kafka.connect.transforms.ExtractField`\\ Value)."

#: ../../source/kafka/documentation.rst:181
msgid ""
"Name Description Type Default Valid Values Importance field Field name to"
" extract. string medium "
"org.apache.kafka.connect.transforms.SetSchemaMetadata Set the schema "
"name, version or both on the record’s key "
"(org.apache.kafka.connect.transforms.SetSchemaMetadata:math:`Key) or "
"value (org.apache.kafka.connect.transforms.SetSchemaMetadata`\\ Value) "
"schema."
msgstr ""

#: ../../source/kafka/documentation.rst:188
msgid ""
"Name Description Type Default Valid Values Importance schema.name Schema "
"name to set. string null high schema.version Schema version to set. int "
"null high org.apache.kafka.connect.transforms.TimestampRouter Update the "
"record’s topic field as a function of the original topic value and the "
"record timestamp."
msgstr "Name Description Type Default Valid Values Importance schema.name Schema name to set. string null high schema.version Schema version to set. int null high org.apache.kafka.connect.transforms.TimestampRouter Update the record’s topic field as a function of the original topic value and the record timestamp."

#: ../../source/kafka/documentation.rst:194
msgid ""
"This is mainly useful for sink connectors, since the topic field is often"
" used to determine the equivalent entity name in the destination "
"system(e.g. database table or search index name)."
msgstr "This is mainly useful for sink connectors, since the topic field is often used to determine the equivalent entity name in the destination system(e.g. database table or search index name)."

#: ../../source/kafka/documentation.rst:198
msgid ""
"Name Description Type Default Valid Values Importance timestamp.format "
"Format string for the timestamp that is compatible with "
"java.text.SimpleDateFormat. string yyyyMMdd high topic.format Format "
"string which can contain ${topic} and ${timestamp} as placeholders for "
"the topic and timestamp, respectively. string :math:`{topic}-`\\ "
"{timestamp} high org.apache.kafka.connect.transforms.RegexRouter Update "
"the record topic using the configured regular expression and replacement "
"string."
msgstr ""

#: ../../source/kafka/documentation.rst:207
msgid ""
"Under the hood, the regex is compiled to a java.util.regex.Pattern. If "
"the pattern matches the input topic, "
"java.util.regex.Matcher#replaceFirst() is used with the replacement "
"string to obtain the new topic."
msgstr "Under the hood, the regex is compiled to a java.util.regex.Pattern. If the pattern matches the input topic, java.util.regex.Matcher#replaceFirst() is used with the replacement string to obtain the new topic."

#: ../../source/kafka/documentation.rst:212
msgid ""
"Name Description Type Default Valid Values Importance regex Regular "
"expression to use for matching. string valid regex high replacement "
"Replacement string. string high "
"org.apache.kafka.connect.transforms.Flatten Flatten a nested data "
"structure, generating names for each field by concatenating the field "
"names at each level with a configurable delimiter character. Applies to "
"Struct when schema present, or a Map in the case of schemaless data. The "
"default delimiter is ‘.’."
msgstr ""

#: ../../source/kafka/documentation.rst:221
msgid ""
"Use the concrete transformation type designed for the record key "
"(org.apache.kafka.connect.transforms.Flatten:math:`Key) or value "
"(org.apache.kafka.connect.transforms.Flatten`\\ Value)."
msgstr "Use the concrete transformation type designed for the record key (org.apache.kafka.connect.transforms.Flatten:math:`Key) or value (org.apache.kafka.connect.transforms.Flatten`\\ Value)."

#: ../../source/kafka/documentation.rst:224
msgid ""
"Name Description Type Default Valid Values Importance delimiter Delimiter"
" to insert between field names from the input record when generating "
"field names for the output record string . medium "
"org.apache.kafka.connect.transforms.Cast Cast fields or the entire key or"
" value to a specific type, e.g. to force an integer field to a smaller "
"width. Only simple primitive types are supported – integers, floats, "
"boolean, and string."
msgstr ""

#: ../../source/kafka/documentation.rst:232
msgid ""
"Use the concrete transformation type designed for the record key "
"(org.apache.kafka.connect.transforms.Cast:math:`Key) or value "
"(org.apache.kafka.connect.transforms.Cast`\\ Value)."
msgstr "Use the concrete transformation type designed for the record key (org.apache.kafka.connect.transforms.Cast:math:`Key) or value (org.apache.kafka.connect.transforms.Cast`\\ Value)."

#: ../../source/kafka/documentation.rst:235
msgid ""
"Name Description Type Default Valid Values Importance spec List of fields"
" and the type to cast them to of the form field1:type,field2:type to cast"
" fields of Maps or Structs. A single type to cast the entire value. Valid"
" types are int8, int16, int32, int64, float32, float64, boolean, and "
"string. list list of colon-delimited pairs, e.g. foo:bar,abc:xyz high "
"org.apache.kafka.connect.transforms.TimestampConverter Convert timestamps"
" between different formats such as Unix epoch, strings, and Connect "
"Date/Timestamp types.Applies to individual fields or to the entire value."
msgstr ""

#: ../../source/kafka/documentation.rst:246
msgid ""
"Use the concrete transformation type designed for the record key "
"(org.apache.kafka.connect.transforms.TimestampConverter:math:`Key) or "
"value (org.apache.kafka.connect.transforms.TimestampConverter`\\ Value)."
msgstr "Use the concrete transformation type designed for the record key (org.apache.kafka.connect.transforms.TimestampConverter:math:`Key) or value (org.apache.kafka.connect.transforms.TimestampConverter`\\ Value)."

#: ../../source/kafka/documentation.rst:249
msgid ""
"Name Description Type Default Valid Values Importance target.type The "
"desired timestamp representation: string, unix, Date, Time, or Timestamp "
"string high field The field containing the timestamp, or empty if the "
"entire value is a timestamp string \"\" high format A SimpleDateFormat-"
"compatible format for the timestamp. Used to generate the output when "
"type=string or used to parse the input if the input is a string. string "
"\"\" medium REST API"
msgstr ""

#: ../../source/kafka/documentation.rst:257
msgid ""
"Since Kafka Connect is intended to be run as a service, it also provides "
"a REST API for managing connectors. The REST API server can be configured"
" using the listeners configuration option. This field should contain a "
"list of listeners in the following format: "
"protocol://host:port,protocol2://host2:port2. Currently supported "
"protocols are http and https. For example: 1"
msgstr ""

#: ../../source/kafka/documentation.rst:264
msgid "listeners=http://localhost:8080,https://localhost:8443"
msgstr "listeners=http://localhost:8080,https://localhost:8443"

#: ../../source/kafka/documentation.rst:266
msgid ""
"By default, if no listeners are specified, the REST server runs on port "
"8083 using the HTTP protocol. When using HTTPS, the configuration has to "
"include the SSL configuration. By default, it will use the ssl.\\* "
"settings. In case it is needed to use different configuration for the "
"REST API than for connecting to Kafka brokers, the fields can be prefixed"
" with listeners.https. When using the prefix, only the prefixed options "
"will be used and the ssl.\\* options without the prefix will be ignored. "
"Following fields can be used to configure HTTPS for the REST API:"
msgstr ""

#: ../../source/kafka/documentation.rst:295
msgid ""
"The REST API is used not only by users to monitor / manage Kafka Connect."
" It is also used for the Kafka Connect cross-cluster communication. "
"Requests received on the follower nodes REST API will be forwarded to the"
" leader node REST API. In case the URI under which is given host "
"reachable is different from the URI which it listens on, the "
"configuration options rest.advertised.host.name, rest.advertised.port and"
" rest.advertised.listener can be used to change the URI which will be "
"used by the follower nodes to connect with the leader. When using both "
"HTTP and HTTPS listeners, the rest.advertised.listener option can be also"
" used to define which listener will be used for the cross-cluster "
"communication. When using HTTPS for communication between nodes, the same"
" ssl.\\* or listeners.https options will be used to configure the HTTPS "
"client."
msgstr ""

#: ../../source/kafka/documentation.rst:309
msgid "The following are the currently supported REST API endpoints:"
msgstr "The following are the currently supported REST API endpoints:"

#: ../../source/kafka/documentation.rst:327
msgid ""
"Kafka Connect also provides a REST API for getting information about "
"connector plugins:"
msgstr "Kafka Connect also provides a REST API for getting information about connector plugins:"

#: ../../source/kafka/documentation.rst:335
msgid "8.3 Connector Development Guide"
msgstr "8.3 Connector Development Guide"

#: ../../source/kafka/documentation.rst:337
msgid ""
"This guide describes how developers can write new connectors for Kafka "
"Connect to move data between Kafka and other systems. It briefly reviews "
"a few key concepts and then describes how to create a simple connector. "
"Core Concepts and APIs Connectors and Tasks"
msgstr "This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector. Core Concepts and APIs Connectors and Tasks"

#: ../../source/kafka/documentation.rst:342
msgid ""
"To copy data between Kafka and another system, users create a Connector "
"for the system they want to pull data from or push data to. Connectors "
"come in two flavors: SourceConnectors import data from another system "
"(e.g. JDBCSourceConnector would import a relational database into Kafka) "
"and SinkConnectors export data (e.g. HDFSSinkConnector would export the "
"contents of a Kafka topic to an HDFS file)."
msgstr ""

#: ../../source/kafka/documentation.rst:349
msgid ""
"Connectors do not perform any data copying themselves: their "
"configuration describes the data to be copied, and the Connector is "
"responsible for breaking that job into a set of Tasks that can be "
"distributed to workers. These Tasks also come in two corresponding "
"flavors: SourceTask and SinkTask."
msgstr "Connectors do not perform any data copying themselves: their configuration describes the data to be copied, and the Connector is responsible for breaking that job into a set of Tasks that can be distributed to workers. These Tasks also come in two corresponding flavors: SourceTask and SinkTask."

#: ../../source/kafka/documentation.rst:355
msgid ""
"With an assignment in hand, each Task must copy its subset of the data to"
" or from Kafka. In Kafka Connect, it should always be possible to frame "
"these assignments as a set of input and output streams consisting of "
"records with consistent schemas. Sometimes this mapping is obvious: each "
"file in a set of log files can be considered a stream with each parsed "
"line forming a record using the same schema and offsets stored as byte "
"offsets in the file. In other cases it may require more effort to map to "
"this model: a JDBC connector can map each table to a stream, but the "
"offset is less clear. One possible mapping uses a timestamp column to "
"generate queries incrementally returning new data, and the last queried "
"timestamp can be used as the offset. Streams and Records"
msgstr ""

#: ../../source/kafka/documentation.rst:367
msgid ""
"Each stream should be a sequence of key-value records. Both the keys and "
"values can have complex structure – many primitive types are provided, "
"but arrays, objects, and nested data structures can be represented as "
"well. The runtime data format does not assume any particular "
"serialization format; this conversion is handled internally by the "
"framework."
msgstr ""

#: ../../source/kafka/documentation.rst:374
msgid ""
"In addition to the key and value, records (both those generated by "
"sources and those delivered to sinks) have associated stream IDs and "
"offsets. These are used by the framework to periodically commit the "
"offsets of data that have been processed so that in the event of "
"failures, processing can resume from the last committed offsets, avoiding"
" unnecessary reprocessing and duplication of events. Dynamic Connectors"
msgstr ""

#: ../../source/kafka/documentation.rst:382
msgid ""
"Not all jobs are static, so Connector implementations are also "
"responsible for monitoring the external system for any changes that might"
" require reconfiguration. For example, in the JDBCSourceConnector "
"example, the Connector might assign a set of tables to each Task. When a "
"new table is created, it must discover this so it can assign the new "
"table to one of the Tasks by updating its configuration. When it notices "
"a change that requires reconfiguration (or a change in the number of "
"Tasks), it notifies the framework and the framework updates any "
"corresponding Tasks. Developing a Simple Connector"
msgstr ""

#: ../../source/kafka/documentation.rst:392
msgid ""
"Developing a connector only requires implementing two interfaces, the "
"Connector and Task. A simple example is included with the source code for"
" Kafka in the file package. This connector is meant for use in standalone"
" mode and has implementations of a SourceConnector/SourceTask to read "
"each line of a file and emit it as a record and a SinkConnector/SinkTask "
"that writes each record to a file."
msgstr ""

#: ../../source/kafka/documentation.rst:399
msgid ""
"The rest of this section will walk through some code to demonstrate the "
"key steps in creating a connector, but developers should also refer to "
"the full example source code as many details are omitted for brevity. "
"Connector Example"
msgstr "The rest of this section will walk through some code to demonstrate the key steps in creating a connector, but developers should also refer to the full example source code as many details are omitted for brevity. Connector Example"

#: ../../source/kafka/documentation.rst:404
msgid ""
"We’ll cover the SourceConnector as a simple example. SinkConnector "
"implementations are very similar. Start by creating the class that "
"inherits from SourceConnector and add a couple of fields that will store "
"parsed configuration information (the filename to read from and the topic"
" to send data to): 1 2 3"
msgstr "We’ll cover the SourceConnector as a simple example. SinkConnector implementations are very similar. Start by creating the class that inherits from SourceConnector and add a couple of fields that will store parsed configuration information (the filename to read from and the topic to send data to): 1 2 3"

#: ../../source/kafka/documentation.rst:410
msgid ""
"public class FileStreamSourceConnector extends SourceConnector { private "
"String filename; private String topic;"
msgstr "public class FileStreamSourceConnector extends SourceConnector { private String filename; private String topic;"

#: ../../source/kafka/documentation.rst:413
msgid ""
"The easiest method to fill in is taskClass(), which defines the class "
"that should be instantiated in worker processes to actually read the "
"data: 1 2 3 4"
msgstr "The easiest method to fill in is taskClass(), which defines the class that should be instantiated in worker processes to actually read the data: 1 2 3 4"

#: ../../source/kafka/documentation.rst:417
msgid ""
"@Override public Class<? extends Task> taskClass() { return "
"FileStreamSourceTask.class; }"
msgstr "@Override public Class<? extends Task> taskClass() { return FileStreamSourceTask.class; }"

#: ../../source/kafka/documentation.rst:420
msgid ""
"We will define the FileStreamSourceTask class below. Next, we add some "
"standard lifecycle methods, start() and stop() : 1 2 3 4 5 6 7 8 9 10 11"
msgstr "We will define the FileStreamSourceTask class below. Next, we add some standard lifecycle methods, start() and stop() : 1 2 3 4 5 6 7 8 9 10 11"

#: ../../source/kafka/documentation.rst:423
msgid ""
"@Override public void start(Map<String, String> props) { // The complete "
"version includes error handling as well. filename = "
"props.get(FILE_CONFIG); topic = props.get(TOPIC_CONFIG); }"
msgstr "@Override public void start(Map<String, String> props) { // The complete version includes error handling as well. filename = props.get(FILE_CONFIG); topic = props.get(TOPIC_CONFIG); }"

#: ../../source/kafka/documentation.rst:427
msgid ""
"@Override public void stop() { // Nothing to do since no background "
"monitoring is required. }"
msgstr "@Override public void stop() { // Nothing to do since no background monitoring is required. }"

#: ../../source/kafka/documentation.rst:430
msgid ""
"Finally, the real core of the implementation is in taskConfigs(). In this"
" case we are only handling a single file, so even though we may be "
"permitted to generate more tasks as per the maxTasks argument, we return "
"a list with only one entry: 1 2 3 4 5 6 7 8 9 10 11"
msgstr "Finally, the real core of the implementation is in taskConfigs(). In this case we are only handling a single file, so even though we may be permitted to generate more tasks as per the maxTasks argument, we return a list with only one entry: 1 2 3 4 5 6 7 8 9 10 11"

#: ../../source/kafka/documentation.rst:435
msgid ""
"@Override public List<Map<String, String>> taskConfigs(int maxTasks) { "
"ArrayList<Map<String, String>> configs = new ArrayList<>(); // Only one "
"input stream makes sense. Map<String, String> config = new HashMap<>(); "
"if (filename != null) config.put(FILE_CONFIG, filename); "
"config.put(TOPIC_CONFIG, topic); configs.add(config); return configs; }"
msgstr "@Override public List<Map<String, String>> taskConfigs(int maxTasks) { ArrayList<Map<String, String>> configs = new ArrayList<>(); // Only one input stream makes sense. Map<String, String> config = new HashMap<>(); if (filename != null) config.put(FILE_CONFIG, filename); config.put(TOPIC_CONFIG, topic); configs.add(config); return configs; }"

#: ../../source/kafka/documentation.rst:441
msgid ""
"Although not used in the example, SourceTask also provides two APIs to "
"commit offsets in the source system: commit and commitRecord. The APIs "
"are provided for source systems which have an acknowledgement mechanism "
"for messages. Overriding these methods allows the source connector to "
"acknowledge messages in the source system, either in bulk or "
"individually, once they have been written to Kafka. The commit API stores"
" the offsets in the source system, up to the offsets that have been "
"returned by poll. The implementation of this API should block until the "
"commit is complete. The commitRecord API saves the offset in the source "
"system for each SourceRecord after it is written to Kafka. As Kafka "
"Connect will record offsets automatically, SourceTasks are not required "
"to implement them. In cases where a connector does need to acknowledge "
"messages in the source system, only one of the APIs is typically "
"required."
msgstr ""

#: ../../source/kafka/documentation.rst:456
msgid ""
"Even with multiple tasks, this method implementation is usually pretty "
"simple. It just has to determine the number of input tasks, which may "
"require contacting the remote service it is pulling data from, and then "
"divvy them up. Because some patterns for splitting work among tasks are "
"so common, some utilities are provided in ConnectorUtils to simplify "
"these cases."
msgstr ""

#: ../../source/kafka/documentation.rst:463
msgid ""
"Note that this simple example does not include dynamic input. See the "
"discussion in the next section for how to trigger updates to task "
"configs. Task Example - Source Task"
msgstr "Note that this simple example does not include dynamic input. See the discussion in the next section for how to trigger updates to task configs. Task Example - Source Task"

#: ../../source/kafka/documentation.rst:467
msgid ""
"Next we’ll describe the implementation of the corresponding SourceTask. "
"The implementation is short, but too long to cover completely in this "
"guide. We’ll use pseudo-code to describe most of the implementation, but "
"you can refer to the source code for the full example."
msgstr "Next we’ll describe the implementation of the corresponding SourceTask. The implementation is short, but too long to cover completely in this guide. We’ll use pseudo-code to describe most of the implementation, but you can refer to the source code for the full example."

#: ../../source/kafka/documentation.rst:472
msgid ""
"Just as with the connector, we need to create a class inheriting from the"
" appropriate base Task class. It also has some standard lifecycle "
"methods: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16"
msgstr "Just as with the connector, we need to create a class inheriting from the appropriate base Task class. It also has some standard lifecycle methods: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16"

#: ../../source/kafka/documentation.rst:476
msgid ""
"public class FileStreamSourceTask extends SourceTask { String filename; "
"InputStream stream; String topic;"
msgstr "public class FileStreamSourceTask extends SourceTask { String filename; InputStream stream; String topic;"

#: ../../source/kafka/documentation.rst:493
msgid ""
"These are slightly simplified versions, but show that these methods "
"should be relatively simple and the only work they should perform is "
"allocating or freeing resources. There are two points to note about this "
"implementation. First, the start() method does not yet handle resuming "
"from a previous offset, which will be addressed in a later section. "
"Second, the stop() method is synchronized. This will be necessary because"
" SourceTasks are given a dedicated thread which they can block "
"indefinitely, so they need to be stopped with a call from a different "
"thread in the Worker."
msgstr ""

#: ../../source/kafka/documentation.rst:503
msgid ""
"Next, we implement the main functionality of the task, the poll() method "
"which gets events from the input system and returns a List: 1 2 3 4 5 6 7"
" 8 9 10 11 12 13 14 15 16 17 18 19 20 21"
msgstr "Next, we implement the main functionality of the task, the poll() method which gets events from the input system and returns a List: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21"

#: ../../source/kafka/documentation.rst:507
msgid ""
"@Override public List poll() throws InterruptedException { try { "
"ArrayList records = new ArrayList<>(); while (streamValid(stream) && "
"records.isEmpty()) { LineAndOffset line = readToNextLine(stream); if "
"(line != null) { Map<String, Object> sourcePartition = "
"Collections.singletonMap(“filename”, filename); Map<String, Object> "
"sourceOffset = Collections.singletonMap(“position”, streamOffset); "
"records.add(new SourceRecord(sourcePartition, sourceOffset, topic, "
"Schema.STRING_SCHEMA, line)); } else { Thread.sleep(1); } } return "
"records; } catch (IOException e) { // Underlying stream was killed, "
"probably as a result of calling stop. Allow to return // null, and "
"driving thread will handle any shutdown if necessary. } return null; }"
msgstr ""

#: ../../source/kafka/documentation.rst:519
msgid ""
"Again, we’ve omitted some details, but we can see the important steps: "
"the poll() method is going to be called repeatedly, and for each call it "
"will loop trying to read records from the file. For each line it reads, "
"it also tracks the file offset. It uses this information to create an "
"output SourceRecord with four pieces of information: the source partition"
" (there is only one, the single file being read), source offset (byte "
"offset in the file), output topic name, and output value (the line, and "
"we include a schema indicating this value will always be a string). Other"
" variants of the SourceRecord constructor can also include a specific "
"output partition, a key, and headers."
msgstr ""

#: ../../source/kafka/documentation.rst:530
msgid ""
"Note that this implementation uses the normal Java InputStream interface "
"and may sleep if data is not available. This is acceptable because Kafka "
"Connect provides each task with a dedicated thread. While task "
"implementations have to conform to the basic poll() interface, they have "
"a lot of flexibility in how they are implemented. In this case, an NIO-"
"based implementation would be more efficient, but this simple approach "
"works, is quick to implement, and is compatible with older versions of "
"Java. Sink Tasks"
msgstr ""

#: ../../source/kafka/documentation.rst:539
msgid ""
"The previous section described how to implement a simple SourceTask. "
"Unlike SourceConnector and SinkConnector, SourceTask and SinkTask have "
"very different interfaces because SourceTask uses a pull interface and "
"SinkTask uses a push interface. Both share the common lifecycle methods, "
"but the SinkTask interface is quite different: 1 2 3 4 5 6 7 8 9"
msgstr "The previous section described how to implement a simple SourceTask. Unlike SourceConnector and SinkConnector, SourceTask and SinkTask have very different interfaces because SourceTask uses a pull interface and SinkTask uses a push interface. Both share the common lifecycle methods, but the SinkTask interface is quite different: 1 2 3 4 5 6 7 8 9"

#: ../../source/kafka/documentation.rst:545
msgid ""
"public abstract class SinkTask implements Task { public void "
"initialize(SinkTaskContext context) { this.context = context; }"
msgstr "public abstract class SinkTask implements Task { public void initialize(SinkTaskContext context) { this.context = context; }"

#: ../../source/kafka/documentation.rst:555
msgid ""
"The SinkTask documentation contains full details, but this interface is "
"nearly as simple as the SourceTask. The put() method should contain most "
"of the implementation, accepting sets of SinkRecords, performing any "
"required translation, and storing them in the destination system. This "
"method does not need to ensure the data has been fully written to the "
"destination system before returning. In fact, in many cases internal "
"buffering will be useful so an entire batch of records can be sent at "
"once, reducing the overhead of inserting events into the downstream data "
"store. The SinkRecords contain essentially the same information as "
"SourceRecords: Kafka topic, partition, offset, the event key and value, "
"and optional headers."
msgstr ""

#: ../../source/kafka/documentation.rst:567
msgid ""
"The flush() method is used during the offset commit process, which allows"
" tasks to recover from failures and resume from a safe point such that no"
" events will be missed. The method should push any outstanding data to "
"the destination system and then block until the write has been "
"acknowledged. The offsets parameter can often be ignored, but is useful "
"in some cases where implementations want to store offset information in "
"the destination store to provide exactly-once delivery. For example, an "
"HDFS connector could do this and use atomic move operations to make sure "
"the flush() operation atomically commits the data and offsets to a final "
"location in HDFS. Resuming from Previous Offsets"
msgstr ""

#: ../../source/kafka/documentation.rst:578
msgid ""
"The SourceTask implementation included a stream ID (the input filename) "
"and offset (position in the file) with each record. The framework uses "
"this to commit offsets periodically so that in the case of a failure, the"
" task can recover and minimize the number of events that are reprocessed "
"and possibly duplicated (or to resume from the most recent offset if "
"Kafka Connect was stopped gracefully, e.g. in standalone mode or due to a"
" job reconfiguration). This commit process is completely automated by the"
" framework, but only the connector knows how to seek back to the right "
"position in the input stream to resume from that location."
msgstr ""

#: ../../source/kafka/documentation.rst:589
msgid ""
"To correctly resume upon startup, the task can use the SourceContext "
"passed into its initialize() method to access the offset data. In "
"initialize(), we would add a bit more code to read the offset (if it "
"exists) and seek to that position: 1 2 3 4 5 6 7"
msgstr "To correctly resume upon startup, the task can use the SourceContext passed into its initialize() method to access the offset data. In initialize(), we would add a bit more code to read the offset (if it exists) and seek to that position: 1 2 3 4 5 6 7"

#: ../../source/kafka/documentation.rst:594
msgid ""
"stream = new FileInputStream(filename); Map<String, Object> offset = "
"context.offsetStorageReader().offset(Collections.singletonMap(FILENAME_FIELD,"
" filename)); if (offset != null) { Long lastRecordedOffset = (Long) "
"offset.get(“position”); if (lastRecordedOffset != null) "
"seekToOffset(stream, lastRecordedOffset); }"
msgstr "stream = new FileInputStream(filename); Map<String, Object> offset = context.offsetStorageReader().offset(Collections.singletonMap(FILENAME_FIELD, filename)); if (offset != null) { Long lastRecordedOffset = (Long) offset.get(“position”); if (lastRecordedOffset != null) seekToOffset(stream, lastRecordedOffset); }"

#: ../../source/kafka/documentation.rst:600
msgid ""
"Of course, you might need to read many keys for each of the input "
"streams. The OffsetStorageReader interface also allows you to issue bulk "
"reads to efficiently load all offsets, then apply them by seeking each "
"input stream to the appropriate position. Dynamic Input/Output Streams"
msgstr "Of course, you might need to read many keys for each of the input streams. The OffsetStorageReader interface also allows you to issue bulk reads to efficiently load all offsets, then apply them by seeking each input stream to the appropriate position. Dynamic Input/Output Streams"

#: ../../source/kafka/documentation.rst:605
msgid ""
"Kafka Connect is intended to define bulk data copying jobs, such as "
"copying an entire database rather than creating many jobs to copy each "
"table individually. One consequence of this design is that the set of "
"input or output streams for a connector can vary over time."
msgstr "Kafka Connect is intended to define bulk data copying jobs, such as copying an entire database rather than creating many jobs to copy each table individually. One consequence of this design is that the set of input or output streams for a connector can vary over time."

#: ../../source/kafka/documentation.rst:610
msgid ""
"Source connectors need to monitor the source system for changes, e.g. "
"table additions/deletions in a database. When they pick up changes, they "
"should notify the framework via the ConnectorContext object that "
"reconfiguration is necessary. For example, in a SourceConnector: 1 2"
msgstr "Source connectors need to monitor the source system for changes, e.g. table additions/deletions in a database. When they pick up changes, they should notify the framework via the ConnectorContext object that reconfiguration is necessary. For example, in a SourceConnector: 1 2"

#: ../../source/kafka/documentation.rst:615
msgid "if (inputsChanged()) this.context.requestTaskReconfiguration();"
msgstr "if (inputsChanged()) this.context.requestTaskReconfiguration();"

#: ../../source/kafka/documentation.rst:617
msgid ""
"The framework will promptly request new configuration information and "
"update the tasks, allowing them to gracefully commit their progress "
"before reconfiguring them. Note that in the SourceConnector this "
"monitoring is currently left up to the connector implementation. If an "
"extra thread is required to perform this monitoring, the connector must "
"allocate it itself."
msgstr ""

#: ../../source/kafka/documentation.rst:624
msgid ""
"Ideally this code for monitoring changes would be isolated to the "
"Connector and tasks would not need to worry about them. However, changes "
"can also affect tasks, most commonly when one of their input streams is "
"destroyed in the input system, e.g. if a table is dropped from a "
"database. If the Task encounters the issue before the Connector, which "
"will be common if the Connector needs to poll for changes, the Task will "
"need to handle the subsequent error. Thankfully, this can usually be "
"handled simply by catching and handling the appropriate exception."
msgstr ""

#: ../../source/kafka/documentation.rst:633
msgid ""
"SinkConnectors usually only have to handle the addition of streams, which"
" may translate to new entries in their outputs (e.g., a new database "
"table). The framework manages any changes to the Kafka input, such as "
"when the set of input topics changes because of a regex subscription. "
"SinkTasks should expect new input streams, which may require creating new"
" resources in the downstream system, such as a new table in a database. "
"The trickiest situation to handle in these cases may be conflicts between"
" multiple SinkTasks seeing a new input stream for the first time and "
"simultaneously trying to create the new resource. SinkConnectors, on the "
"other hand, will generally require no special code for handling a dynamic"
" set of streams. Connect Configuration Validation"
msgstr ""

#: ../../source/kafka/documentation.rst:646
msgid ""
"Kafka Connect allows you to validate connector configurations before "
"submitting a connector to be executed and can provide feedback about "
"errors and recommended values. To take advantage of this, connector "
"developers need to provide an implementation of config() to expose the "
"configuration definition to the framework."
msgstr "Kafka Connect allows you to validate connector configurations before submitting a connector to be executed and can provide feedback about errors and recommended values. To take advantage of this, connector developers need to provide an implementation of config() to expose the configuration definition to the framework."

#: ../../source/kafka/documentation.rst:652
msgid ""
"The following code in FileStreamSourceConnector defines the configuration"
" and exposes it to the framework. 1 2 3 4 5 6 7"
msgstr "The following code in FileStreamSourceConnector defines the configuration and exposes it to the framework. 1 2 3 4 5 6 7"

#: ../../source/kafka/documentation.rst:655
msgid ""
"private static final ConfigDef CONFIG_DEF = new ConfigDef() "
".define(FILE_CONFIG, Type.STRING, Importance.HIGH, “Source filename.”) "
".define(TOPIC_CONFIG, Type.STRING, Importance.HIGH, “The topic to publish"
" data to”);"
msgstr "private static final ConfigDef CONFIG_DEF = new ConfigDef() .define(FILE_CONFIG, Type.STRING, Importance.HIGH, “Source filename.”) .define(TOPIC_CONFIG, Type.STRING, Importance.HIGH, “The topic to publish data to”);"

#: ../../source/kafka/documentation.rst:660
msgid "public ConfigDef config() { return CONFIG_DEF; }"
msgstr "public ConfigDef config() { return CONFIG_DEF; }"

#: ../../source/kafka/documentation.rst:662
msgid ""
"ConfigDef class is used for specifying the set of expected "
"configurations. For each configuration, you can specify the name, the "
"type, the default value, the documentation, the group information, the "
"order in the group, the width of the configuration value and the name "
"suitable for display in the UI. Plus, you can provide special validation "
"logic used for single configuration validation by overriding the "
"Validator class. Moreover, as there may be dependencies between "
"configurations, for example, the valid values and visibility of a "
"configuration may change according to the values of other configurations."
" To handle this, ConfigDef allows you to specify the dependents of a "
"configuration and to provide an implementation of Recommender to get "
"valid values and set visibility of a configuration given the current "
"configuration values."
msgstr ""

#: ../../source/kafka/documentation.rst:676
msgid ""
"Also, the validate() method in Connector provides a default validation "
"implementation which returns a list of allowed configurations together "
"with configuration errors and recommended values for each configuration. "
"However, it does not use the recommended values for configuration "
"validation. You may provide an override of the default implementation for"
" customized configuration validation, which may use the recommended "
"values. Working with Schemas"
msgstr ""

#: ../../source/kafka/documentation.rst:684
msgid ""
"The FileStream connectors are good examples because they are simple, but "
"they also have trivially structured data – each line is just a string. "
"Almost all practical connectors will need schemas with more complex data "
"formats."
msgstr "The FileStream connectors are good examples because they are simple, but they also have trivially structured data – each line is just a string. Almost all practical connectors will need schemas with more complex data formats."

#: ../../source/kafka/documentation.rst:689
msgid ""
"To create more complex data, you’ll need to work with the Kafka Connect "
"data API. Most structured records will need to interact with two classes "
"in addition to primitive types: Schema and Struct."
msgstr "To create more complex data, you’ll need to work with the Kafka Connect data API. Most structured records will need to interact with two classes in addition to primitive types: Schema and Struct."

#: ../../source/kafka/documentation.rst:693
msgid ""
"The API documentation provides a complete reference, but here is a simple"
" example creating a Schema and Struct: 1 2 3 4 5 6 7 8 9"
msgstr "The API documentation provides a complete reference, but here is a simple example creating a Schema and Struct: 1 2 3 4 5 6 7 8 9"

#: ../../source/kafka/documentation.rst:696
msgid ""
"Schema schema = SchemaBuilder.struct().name(NAME) .field(“name”, "
"Schema.STRING_SCHEMA) .field(“age”, Schema.INT_SCHEMA) .field(“admin”, "
"new SchemaBuilder.boolean().defaultValue(false).build()) .build();"
msgstr "Schema schema = SchemaBuilder.struct().name(NAME) .field(“name”, Schema.STRING_SCHEMA) .field(“age”, Schema.INT_SCHEMA) .field(“admin”, new SchemaBuilder.boolean().defaultValue(false).build()) .build();"

#: ../../source/kafka/documentation.rst:700
msgid ""
"Struct struct = new Struct(schema) .put(“name”, “Barbara Liskov”) "
".put(“age”, 75);"
msgstr "Struct struct = new Struct(schema) .put(“name”, “Barbara Liskov”) .put(“age”, 75);"

#: ../../source/kafka/documentation.rst:703
msgid ""
"If you are implementing a source connector, you’ll need to decide when "
"and how to create schemas. Where possible, you should avoid recomputing "
"them as much as possible. For example, if your connector is guaranteed to"
" have a fixed schema, create it statically and reuse a single instance."
msgstr "If you are implementing a source connector, you’ll need to decide when and how to create schemas. Where possible, you should avoid recomputing them as much as possible. For example, if your connector is guaranteed to have a fixed schema, create it statically and reuse a single instance."

#: ../../source/kafka/documentation.rst:709
msgid ""
"However, many connectors will have dynamic schemas. One simple example of"
" this is a database connector. Considering even just a single table, the "
"schema will not be predefined for the entire connector (as it varies from"
" table to table). But it also may not be fixed for a single table over "
"the lifetime of the connector since the user may execute an ALTER TABLE "
"command. The connector must be able to detect these changes and react "
"appropriately."
msgstr ""

#: ../../source/kafka/documentation.rst:717
msgid ""
"Sink connectors are usually simpler because they are consuming data and "
"therefore do not need to create schemas. However, they should take just "
"as much care to validate that the schemas they receive have the expected "
"format. When the schema does not match – usually indicating the upstream "
"producer is generating invalid data that cannot be correctly translated "
"to the destination system – sink connectors should throw an exception to "
"indicate this error to the system. Kafka Connect Administration"
msgstr ""

#: ../../source/kafka/documentation.rst:725
msgid ""
"Kafka Connect’s REST layer provides a set of APIs to enable "
"administration of the cluster. This includes APIs to view the "
"configuration of connectors and the status of their tasks, as well as to "
"alter their current behavior (e.g. changing configuration and restarting "
"tasks)."
msgstr "Kafka Connect’s REST layer provides a set of APIs to enable administration of the cluster. This includes APIs to view the configuration of connectors and the status of their tasks, as well as to alter their current behavior (e.g. changing configuration and restarting tasks)."

#: ../../source/kafka/documentation.rst:731
msgid ""
"When a connector is first submitted to the cluster, the workers rebalance"
" the full set of connectors in the cluster and their tasks so that each "
"worker has approximately the same amount of work. This same rebalancing "
"procedure is also used when connectors increase or decrease the number of"
" tasks they require, or when a connector’s configuration is changed. You "
"can use the REST API to view the current status of a connector and its "
"tasks, including the id of the worker to which each was assigned. For "
"example, querying the status of a file source (using GET /connectors"
"/file-source/status) might produce output like the following: 1 2 3 4 5 6"
" 7 8 9 10 11 12 13 14"
msgstr ""

#: ../../source/kafka/documentation.rst:742
msgid ""
"{ “name”: “file-source”, “connector”: { “state”: “RUNNING”, “worker_id”: "
"“192.168.1.208:8083” }, “tasks”: [ { “id”: 0, “state”: “RUNNING”, "
"“worker_id”: “192.168.1.209:8083” }] }"
msgstr "{ “name”: “file-source”, “connector”: { “state”: “RUNNING”, “worker_id”: “192.168.1.208:8083” }, “tasks”: [ { “id”: 0, “state”: “RUNNING”, “worker_id”: “192.168.1.209:8083” }] }"

#: ../../source/kafka/documentation.rst:746
msgid ""
"Connectors and their tasks publish status updates to a shared topic "
"(configured with status.storage.topic) which all workers in the cluster "
"monitor. Because the workers consume this topic asynchronously, there is "
"typically a (short) delay before a state change is visible through the "
"status API. The following states are possible for a connector or one of "
"its tasks:"
msgstr ""

#: ../../source/kafka/documentation.rst:760
msgid ""
"In most cases, connector and task states will match, though they may be "
"different for short periods of time when changes are occurring or if "
"tasks have failed. For example, when a connector is first started, there "
"may be a noticeable delay before the connector and its tasks have all "
"transitioned to the RUNNING state. States will also diverge when tasks "
"fail since Connect does not automatically restart failed tasks. To "
"restart a connector/task manually, you can use the restart APIs listed "
"above. Note that if you try to restart a task while a rebalance is taking"
" place, Connect will return a 409 (Conflict) status code. You can retry "
"after the rebalance completes, but it might not be necessary since "
"rebalances effectively restart all the connectors and tasks in the "
"cluster."
msgstr ""

#: ../../source/kafka/documentation.rst:773
msgid ""
"It’s sometimes useful to temporarily stop the message processing of a "
"connector. For example, if the remote system is undergoing maintenance, "
"it would be preferable for source connectors to stop polling it for new "
"data instead of filling logs with exception spam. For this use case, "
"Connect offers a pause/resume API. While a source connector is paused, "
"Connect will stop polling it for additional records. While a sink "
"connector is paused, Connect will stop pushing new messages to it. The "
"pause state is persistent, so even if you restart the cluster, the "
"connector will not begin message processing again until the task has been"
" resumed. Note that there may be a delay before all of a connector’s "
"tasks have transitioned to the PAUSED state since it may take time for "
"them to finish whatever processing they were in the middle of when being "
"paused. Additionally, failed tasks will not transition to the PAUSED "
"state until they have been restarted. 9. Kafka Streams"
msgstr ""

#: ../../source/kafka/documentation.rst:788
msgid ""
"Kafka Streams is a client library for processing and analyzing data "
"stored in Kafka. It builds upon important stream processing concepts such"
" as properly distinguishing between event time and processing time, "
"windowing support, exactly-once processing semantics and simple yet "
"efficient management of application state."
msgstr "Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, exactly-once processing semantics and simple yet efficient management of application state."

#: ../../source/kafka/documentation.rst:794
msgid ""
"Kafka Streams has a low barrier to entry: You can quickly write and run a"
" small-scale proof-of-concept on a single machine; and you only need to "
"run additional instances of your application on multiple machines to "
"scale up to high-volume production workloads. Kafka Streams transparently"
" handles the load balancing of multiple instances of the same application"
" by leveraging Kafka’s parallelism model."
msgstr ""

#: ../../source/kafka/documentation.rst:801
msgid "Learn More about Kafka Streams read this Section."
msgstr "Learn More about Kafka Streams read this Section."

